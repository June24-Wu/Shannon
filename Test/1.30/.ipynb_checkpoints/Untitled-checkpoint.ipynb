{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa593762",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T08:13:42.060903Z",
     "start_time": "2022-01-30T08:13:42.057501Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_LSTM(nn.Module):\n",
    "    def __init__(self,dimention,factor_num, sequence, fully_connect_layer_neural):\n",
    "        super(Transformer_LSTM, self).__init__()\n",
    "        self.factor_num = factor_num  # 108\n",
    "        self.sequence = sequence # \n",
    "        self.dimention = dimention # \n",
    "        self.fc2_neuron = fully_connect_layer_neural  # 32\n",
    "\n",
    "        # Layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.LeakyReLU = nn.LeakyReLU()\n",
    "        self.out = nn.Linear(self.fc2_neuron*2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = torch.transpose(x, 0,1)  # x.shape: torch.Size([3, 512, 20, 108])\n",
    "        \n",
    "        start, (hn, cn) = self.transformer_lstm(x[0],2) # torch.Size([512, 20, 128])\n",
    "        for i in range(1,x.shape[0]):\n",
    "            add = self.transformer_lstm(x[i],2)\n",
    "            start += add\n",
    "            start = self.norm(start)\n",
    "        # start = torch.Size([512, 20, 128])\n",
    "        x, _ = self.attention_net(start, hn)\n",
    "        x = self.norm(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.dropout(x)\n",
    "        y_pred = self.out(x)\n",
    "        return y_pred\n",
    "    def norm(self,x):\n",
    "        batch_num ,original_shape= x.shape[0] , x.shape\n",
    "        x = x.reshape(batch_num, -1).float()\n",
    "        x = torch.nn.BatchNorm1d(x.shape[-1])(x)\n",
    "        x = x.reshape(original_shape)\n",
    "        return x\n",
    "    def transformer_lstm(self,x,layer_num):\n",
    "        q = nn.Linear(self.factor_num, self.factor_num)(x)\n",
    "        k = nn.Linear(self.factor_num, self.factor_num)(x)\n",
    "        v = nn.Linear(self.factor_num, self.factor_num)(x)\n",
    "        x, x_weight = nn.MultiheadAttention(self.factor_num, layer_num,batch_first=True)(q,k,v) # attn_output = torch.Size([512, 20, 128])\n",
    "        # out = torch.Size([512, 20, 128])\n",
    "        out, (hn,cn) = torch.nn.LSTM(self.factor_num, self.fc2_neuron, layer_num, batch_first=True, bidirectional=True, dropout=0.2)(x)\n",
    "        return out, (hn,cn)\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        # lstm_output : [batch_size, n_step, n_hidden * num_directions(=2)], F matrix\n",
    "        # final_state : [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "\n",
    "        hidden = torch.cat((final_state[0], final_state[1]), dim=1).unsqueeze(\n",
    "            2)  # hidden : [batch_size, n_hidden * num_directions(=2), n_layer(=1)]\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2)  # [batch_size,sequence]\n",
    "        attn_weights = torch.nn.functional.softmax(attn_weights, 1)  # [batch_size,sequence]   # torch.Size([512, 20])\n",
    "        # context: [batch_size, n_hidden * num_directions(=2)]\n",
    "        output = torch.bmm(lstm_output.transpose(1, 2), attn_weights.unsqueeze(2)).squeeze(\n",
    "            2)  # [batch_size, n_hidden * num_directions(=2)]\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1609e9fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T09:25:53.469480Z",
     "start_time": "2022-01-30T09:25:52.898173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainx.shape:  torch.Size([10000, 3, 20, 108])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from progressbar import ProgressBar\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "trainx = torch.randn(10000,3,20,108)\n",
    "# trainx = torch.randn(10000,20,108)\n",
    "trainy = torch.randn(10000,1)\n",
    "print(\"trainx.shape: \" , trainx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c2dbe142",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T09:25:53.474591Z",
     "start_time": "2022-01-30T09:25:53.471335Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = Data.TensorDataset(trainx, trainy)\n",
    "batch_size = 1024\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=16,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "eaf9044e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T10:03:17.704678Z",
     "start_time": "2022-01-30T10:03:17.669572Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer_LSTM(nn.Module):\n",
    "    def __init__(self,dimention,factor_num, sequence, fully_connect_layer_neural,device,layer_num=2):\n",
    "        super(Transformer_LSTM, self).__init__()\n",
    "        self.factor_num = factor_num  # 108\n",
    "        self.sequence = sequence # \n",
    "        self.dimention = dimention # \n",
    "        self.fc2_neuron = fully_connect_layer_neural  # 32\n",
    "        self.device = device\n",
    "        \n",
    "        # Layer\n",
    "        self.bn1 = torch.nn.BatchNorm1d(self.dimention * self.factor_num* self.sequence)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(self.fc2_neuron*2* self.sequence)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(self.fc2_neuron*2)\n",
    "        self.q_metrix = nn.Linear(self.factor_num, self.factor_num)\n",
    "        self.k_metrix = nn.Linear(self.factor_num, self.factor_num)\n",
    "        self.v_metrix = nn.Linear(self.factor_num, self.factor_num)\n",
    "        self.MultiheadAttention = nn.MultiheadAttention(self.factor_num,layer_num,batch_first=True)\n",
    "        self.lstm = nn.LSTM(self.factor_num, self.fc2_neuron, layer_num, batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.LeakyReLU = nn.LeakyReLU()\n",
    "        self.out = nn.Linear(self.fc2_neuron*2, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.norm_1(x)\n",
    "        x = torch.transpose(x, 0,1)  # x.shape: torch.Size([3, 512, 20, 108])\n",
    "        \n",
    "        final, (hn, cn) = self.transformer_lstm(x[0],2) # torch.Size([512, 20, 128])\n",
    "        for i in range(1,x.shape[0]):\n",
    "            add , _ = self.transformer_lstm(x[i],2)\n",
    "            final = self.skip_connection(final,add)\n",
    "        # start = torch.Size([512, 20, 128])\n",
    "        \n",
    "        x, _ = self.attention_net(final, hn)\n",
    "        x = self.bn3(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.dropout(x)\n",
    "        y_pred = self.out(x)\n",
    "        return y_pred\n",
    "    def norm_1(self,x):\n",
    "        batch_num ,original_shape= x.shape[0] , x.shape\n",
    "        x = x.reshape(batch_num, -1)\n",
    "        x = self.bn1(x)\n",
    "        x = x.reshape(original_shape)\n",
    "        return x\n",
    "    def norm_2(self,x):\n",
    "        batch_num ,original_shape= x.shape[0] , x.shape\n",
    "        x = x.reshape(batch_num, -1)\n",
    "        x = self.bn2(x)\n",
    "        x = x.reshape(original_shape)\n",
    "        return x\n",
    "    def transformer_lstm(self,x,layer_num):\n",
    "        q = self.q_metrix(x)\n",
    "        k = self.k_metrix(x)\n",
    "        v = self.v_metrix(x)\n",
    "        x, x_weight = self.MultiheadAttention(q,k,v) # attn_output = torch.Size([512, 20, 128])\n",
    "        # out = torch.Size([512, 20, 128])\n",
    "        out, (hn,cn) = self.lstm(x)\n",
    "        return out, (hn,cn)\n",
    "    def skip_connection(self,origin,add):\n",
    "        return self.norm_2(origin + add)\n",
    "        \n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        # lstm_output : [batch_size, n_step, n_hidden * num_directions(=2)], F matrix\n",
    "        # final_state : [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "\n",
    "        hidden = torch.cat((final_state[0], final_state[1]), dim=1).unsqueeze(\n",
    "            2)  # hidden : [batch_size, n_hidden * num_directions(=2), n_layer(=1)]\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2)  # [batch_size,sequence]\n",
    "        attn_weights = torch.nn.functional.softmax(attn_weights, 1)  # [batch_size,sequence]   # torch.Size([512, 20])\n",
    "        # context: [batch_size, n_hidden * num_directions(=2)]\n",
    "        output = torch.bmm(lstm_output.transpose(1, 2), attn_weights.unsqueeze(2)).squeeze(\n",
    "            2)  # [batch_size, n_hidden * num_directions(=2)]\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc687a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-30T10:03:17.830Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer_LSTM(\n",
      "  (bn1): BatchNorm1d(6480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (q_metrix): Linear(in_features=108, out_features=108, bias=True)\n",
      "  (k_metrix): Linear(in_features=108, out_features=108, bias=True)\n",
      "  (v_metrix): Linear(in_features=108, out_features=108, bias=True)\n",
      "  (MultiheadAttention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=108, out_features=108, bias=True)\n",
      "  )\n",
      "  (lstm): LSTM(108, 64, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (LeakyReLU): LeakyReLU(negative_slope=0.01)\n",
      "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuwenjun/venv-jupyter/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  2%|▏         | 1/50 [00:03<03:05,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  loss:  1.1999158142089843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 2/50 [00:08<03:17,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2  loss:  1.0569059387207032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 3/50 [00:12<03:20,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3  loss:  1.0411659729003906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 4/50 [00:16<03:17,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4  loss:  1.0380450561523438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 5/50 [00:21<03:13,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5  loss:  1.035803271484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 6/50 [00:25<03:10,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6  loss:  1.0385421691894532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 7/50 [00:30<03:09,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7  loss:  1.0390138854980469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 8/50 [00:34<03:04,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8  loss:  1.038879052734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 9/50 [00:38<03:00,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9  loss:  1.0426197326660156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 10/50 [00:43<02:58,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10  loss:  1.0480241149902343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 11/50 [00:47<02:52,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11  loss:  1.0468027954101562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 12/50 [00:52<02:49,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12  loss:  1.0438993957519531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 13/50 [00:56<02:44,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13  loss:  1.0458595642089843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 14/50 [01:01<02:40,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14  loss:  1.0413813537597656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 15/50 [01:05<02:33,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15  loss:  1.0417045837402343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 16/50 [01:09<02:29,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16  loss:  1.0449946838378905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 17/50 [01:14<02:26,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17  loss:  1.0432783142089843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 18/50 [01:19<02:23,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18  loss:  1.0439300537109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 19/50 [01:23<02:18,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19  loss:  1.042217803955078\n"
     ]
    }
   ],
   "source": [
    "alphanet = Transformer_LSTM(3,108,20,64,\"cuda:0\")\n",
    "# alphanet = AlphaNet_LSTM_V1(108,20,64)\n",
    "alphanet = alphanet.cuda()\n",
    "# alphanet = torch.nn.parallel.DataParallel(alphanet)\n",
    "print(alphanet)\n",
    "total_length = trainx.shape[0]\n",
    "LR = 0.01\n",
    "loss_function = nn.MSELoss().cuda()\n",
    "optimizer = optim.Adam(alphanet.parameters(), lr=LR)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=2,gamma = 0.5)\n",
    "epoch_num = 50\n",
    "loss_list = []\n",
    "\n",
    "min_loss = float(\"inf\")\n",
    "for epoch in tqdm(range(epoch_num)):\n",
    "    total_loss = 0\n",
    "    for _, (inputs, outputs) in enumerate(train_loader):\n",
    "        inputs = Variable(inputs).float().cuda()\n",
    "        outputs = Variable(outputs).float().cuda()\n",
    "        optimizer.zero_grad() # noticed:  the grad return to zero before starting the loop\n",
    "        \n",
    "        # forward + backward +update\n",
    "        pred = alphanet(inputs.cuda())\n",
    "        pred = pred.cuda()\n",
    "        loss = loss_function(pred, outputs)\n",
    "        with torch.autograd.detect_anomaly():\n",
    "            loss.backward()\n",
    "#         loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         lr_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "        total_loss += loss.item()\n",
    "    total_loss = total_loss * batch_size / total_length\n",
    "    print('Epoch: ', epoch + 1, ' loss: ', total_loss)\n",
    "    loss_list.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bd1df061",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T09:31:39.570894Z",
     "start_time": "2022-01-30T09:31:39.563404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ae2828b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T09:15:07.707765Z",
     "start_time": "2022-01-30T09:15:07.699391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8269e-01, -5.0047e-01,  3.3323e-02,  8.6941e-01],\n",
       "        [ 3.2009e-02,  7.1281e-04,  1.2799e+00, -3.1223e-01],\n",
       "        [-1.9265e-01,  8.5219e-01,  4.5535e-01, -1.4224e+00]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a.reshape(3,-1)\n",
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4d45d5ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T09:04:36.605938Z",
     "start_time": "2022-01-30T09:04:36.596542Z"
    }
   },
   "outputs": [],
   "source": [
    "def transformer_lstm(x,layer_num):\n",
    "    q = nn.Linear(factor_num, factor_num)(x)\n",
    "    k = nn.Linear(factor_num, factor_num)(x)\n",
    "    v = nn.Linear(factor_num, factor_num)(x)\n",
    "    x, x_weight = nn.MultiheadAttention(factor_num, layer_num,batch_first=True)(q,k,v) # attn_output = torch.Size([512, 20, 128])\n",
    "    # out = torch.Size([512, 20, 128])\n",
    "    out, (hn,cn) = torch.nn.LSTM(factor_num, 64, layer_num, batch_first=True, bidirectional=True, dropout=0.2)(x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "94e941c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T09:08:24.958185Z",
     "start_time": "2022-01-30T09:08:24.610653Z"
    }
   },
   "outputs": [],
   "source": [
    "start = transformer_lstm(x[0],2) # torch.Size([512, 20, 128])\n",
    "for i in range(1,x.shape[0]):\n",
    "    add = transformer_lstm(x[i],2)\n",
    "    start += add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cf5985d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T09:09:07.825958Z",
     "start_time": "2022-01-30T09:09:07.819066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 20, 128])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ff09ee0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T09:05:06.258197Z",
     "start_time": "2022-01-30T09:05:06.249366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 20, 128])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "875ff401",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T08:57:36.789020Z",
     "start_time": "2022-01-30T08:57:36.783215Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_num ,original_shape= x.shape[0] , x.shape\n",
    "x = x.reshape(batch_num, -1).float()\n",
    "# x = torch.nn.BatchNorm1d(x.shape[-1])(x)\n",
    "x = x.reshape(original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "898f8718",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T08:57:38.882867Z",
     "start_time": "2022-01-30T08:57:38.871974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3720, -0.8041, -0.9032, -0.6947],\n",
       "          [ 0.0581, -0.5880,  0.8210, -0.5347],\n",
       "          [ 1.5516, -2.1451,  1.2694, -0.2031]],\n",
       "\n",
       "         [[-0.4983, -0.3954,  0.1497, -0.3727],\n",
       "          [ 2.2113, -1.0530, -0.4277,  0.0852],\n",
       "          [ 0.7044, -2.0854, -1.0672,  0.2603]],\n",
       "\n",
       "         [[-2.0598, -0.3966,  1.0161,  0.4361],\n",
       "          [-0.8015, -0.6283, -2.2997,  1.4656],\n",
       "          [ 0.0649, -0.5740,  0.7617, -0.2757]]],\n",
       "\n",
       "\n",
       "        [[[-0.5442, -0.9948,  0.8970, -0.2814],\n",
       "          [-0.8861,  0.0641,  0.1193, -0.4461],\n",
       "          [ 0.0212, -0.2006, -0.3645,  0.6403]],\n",
       "\n",
       "         [[-1.7014, -0.5908,  0.8032, -0.5609],\n",
       "          [-0.4263,  0.0032, -0.8050, -1.1229],\n",
       "          [-0.6438,  0.5535,  0.0984,  1.1140]],\n",
       "\n",
       "         [[ 1.1569,  1.1844, -0.9598, -1.7448],\n",
       "          [-0.2499,  0.2846, -0.1533, -1.4458],\n",
       "          [ 0.2298, -2.0872,  0.1569,  1.0752]]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "827671e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T08:56:35.619998Z",
     "start_time": "2022-01-30T08:56:35.613815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 20, 108])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5dc98325",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T08:52:59.057960Z",
     "start_time": "2022-01-30T08:52:59.034640Z"
    }
   },
   "outputs": [],
   "source": [
    "x, x_weight = nn.MultiheadAttention(factor_num, 2,batch_first=True)(q,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b56be573",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T08:11:02.098837Z",
     "start_time": "2022-01-30T08:11:01.935285Z"
    }
   },
   "outputs": [],
   "source": [
    "out, (hn,cn) = torch.nn.LSTM(108, 64, 2, batch_first=True, bidirectional=True, dropout=0.2)(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acbac057",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T08:40:06.477508Z",
     "start_time": "2022-01-30T08:40:06.445591Z"
    }
   },
   "outputs": [],
   "source": [
    "multihead_attn = nn.MultiheadAttention(128, 4,batch_first=True)\n",
    "attn_output, attn_output_weights = multihead_attn(out,out,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "792a2b41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T08:40:08.308220Z",
     "start_time": "2022-01-30T08:40:08.300472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 20, 128])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "277f9776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T08:48:08.769187Z",
     "start_time": "2022-01-30T08:48:08.749636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 20, 128])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(128, 128)(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e059bb20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T08:40:36.123507Z",
     "start_time": "2022-01-30T08:40:36.116028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 20, 20])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e62c7d43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T08:06:29.631780Z",
     "start_time": "2022-01-30T08:06:29.619881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5335, -1.8345],\n",
       "         [-0.4648,  0.3510],\n",
       "         [-0.8114, -0.6642],\n",
       "         [-1.6805,  0.6834]],\n",
       "\n",
       "        [[ 0.9820, -1.5513],\n",
       "         [-2.2197,  0.6072],\n",
       "         [ 1.0861,  0.8014],\n",
       "         [ 0.6801,  0.5972]],\n",
       "\n",
       "        [[-0.9948, -0.0429],\n",
       "         [-0.0649, -0.1456],\n",
       "         [-0.1955,  1.2058],\n",
       "         [ 0.0729, -1.0356]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reshape(3,4,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
