{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4517731a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T10:56:03.355968Z",
     "start_time": "2022-01-30T10:56:00.909956Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da363f9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T11:03:41.195764Z",
     "start_time": "2022-01-30T11:03:41.159822Z"
    }
   },
   "outputs": [],
   "source": [
    "class Res_LSTM(nn.Module):\n",
    "    def __init__(self, dimention, factor_num, sequence, fully_connect_layer_neural, layer_num=2,transformer = False):\n",
    "        super(Res_LSTM, self).__init__()\n",
    "        self.factor_num = factor_num  # 108\n",
    "        self.sequence = sequence  #\n",
    "        self.dimention = dimention  #\n",
    "        self.fc2_neuron = fully_connect_layer_neural  # 32\n",
    "        self.transformer = transformer\n",
    "\n",
    "        # Layer\n",
    "        self.bn1 = torch.nn.BatchNorm1d(self.dimention * self.factor_num * self.sequence)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(self.fc2_neuron * 2 * self.sequence)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(self.fc2_neuron * 2)\n",
    "        if self.transformer == True:\n",
    "            self.q_metrix = nn.Linear(self.factor_num, self.factor_num)\n",
    "            self.k_metrix = nn.Linear(self.factor_num, self.factor_num)\n",
    "            self.v_metrix = nn.Linear(self.factor_num, self.factor_num)\n",
    "            self.MultiheadAttention = nn.MultiheadAttention(self.factor_num, layer_num, batch_first=True)\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.factor_num, self.fc2_neuron, layer_num, batch_first=True, bidirectional=True,\n",
    "                            dropout=0.2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.LeakyReLU = nn.LeakyReLU()\n",
    "        self.out = nn.Linear(self.fc2_neuron * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm_1(x)\n",
    "        x = torch.transpose(x, 0, 1)  # x.shape: torch.Size([3, 512, 20, 108])\n",
    "\n",
    "        final, (hn, cn) = self.lstm_layer(x[0], 2)  # torch.Size([512, 20, 128])\n",
    "        for i in range(1, x.shape[0]):\n",
    "            add, _ = self.lstm_layer(x[i], 2)\n",
    "            final = self.skip_connection(final, add)\n",
    "        # start = torch.Size([512, 20, 128])\n",
    "\n",
    "        x, _ = self.attention_net(final, hn)\n",
    "        x = self.bn3(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.dropout(x)\n",
    "        y_pred = self.out(x)\n",
    "        return y_pred\n",
    "\n",
    "    def norm_1(self, x):\n",
    "        batch_num, original_shape = x.shape[0], x.shape\n",
    "        x = x.reshape(batch_num, -1)\n",
    "        x = self.bn1(x)\n",
    "        x = x.reshape(original_shape)\n",
    "        return x\n",
    "\n",
    "    def norm_2(self, x):\n",
    "        batch_num, original_shape = x.shape[0], x.shape\n",
    "        x = x.reshape(batch_num, -1)\n",
    "        x = self.bn2(x)\n",
    "        x = x.reshape(original_shape)\n",
    "        return x\n",
    "\n",
    "    def lstm_layer(self, x, layer_num):\n",
    "        if self.transformer == True:\n",
    "            q = self.q_metrix(x)\n",
    "            k = self.k_metrix(x)\n",
    "            v = self.v_metrix(x)\n",
    "            x, x_weight = self.MultiheadAttention(q, k, v)  # attn_output = torch.Size([512, 20, 128])\n",
    "        else:\n",
    "            pass\n",
    "        # out = torch.Size([512, 20, 128])\n",
    "        out, (hn, cn) = self.lstm(x)\n",
    "        return out,(hn, cn)\n",
    "\n",
    "    def skip_connection(self, origin, add):\n",
    "        return self.norm_2(origin + add)\n",
    "\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        # lstm_output : [batch_size, n_step, n_hidden * num_directions(=2)], F matrix\n",
    "        # final_state : [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "\n",
    "        hidden = torch.cat((final_state[0], final_state[1]), dim=1).unsqueeze(\n",
    "            2)  # hidden : [batch_size, n_hidden * num_directions(=2), n_layer(=1)]\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2)  # [batch_size,sequence]\n",
    "        attn_weights = torch.nn.functional.softmax(attn_weights, 1)  # [batch_size,sequence]   # torch.Size([512, 20])\n",
    "        # context: [batch_size, n_hidden * num_directions(=2)]\n",
    "        output = torch.bmm(lstm_output.transpose(1, 2), attn_weights.unsqueeze(2)).squeeze(\n",
    "            2)  # [batch_size, n_hidden * num_directions(=2)]\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d8f3106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T11:15:20.318159Z",
     "start_time": "2022-01-30T11:15:20.289610Z"
    }
   },
   "outputs": [],
   "source": [
    "class AlphaNet_LSTM_V1(nn.Module):\n",
    "    def __init__(self, factor_num, sequence, fully_connect_layer_neural, attention=False,transformer = False):\n",
    "        super(AlphaNet_LSTM_V1, self).__init__()\n",
    "        self.factor_num = factor_num  # 108\n",
    "        self.sequence = sequence\n",
    "        self.fc2_neuron = fully_connect_layer_neural  # 32\n",
    "        self.attention = attention\n",
    "        self.transformer = transformer\n",
    "        # Layer\n",
    "        self.batch = torch.nn.BatchNorm1d(self.sequence * self.factor_num)\n",
    "        self.lstm = nn.LSTM(self.factor_num, self.fc2_neuron, 3, batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        self.lstm2 = nn.LSTM(int(self.fc2_neuron * 2), int(self.fc2_neuron / 2), 3, batch_first=True,\n",
    "                             bidirectional=True, dropout=0.2)\n",
    "        self.batch2 = torch.nn.BatchNorm1d(int(self.fc2_neuron * 2))\n",
    "        self.batch3 = torch.nn.BatchNorm1d(self.fc2_neuron)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.LeakyReLU = nn.LeakyReLU()\n",
    "        self.out = nn.Linear(self.fc2_neuron, 1)\n",
    "        if self.transformer == True:\n",
    "            self.TransformerLayer = nn.TransformerEncoderLayer(d_model=self.fc2_neuron * 2, nhead=2,batch_first=True)\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], -1).float()\n",
    "        x = self.batch(x)\n",
    "        x = x.reshape(x.shape[0], self.sequence, self.factor_num)\n",
    "        \n",
    "        \n",
    "        x, _ = self.lstm(x)  # x.shape: torch.Size([6182, 10, 128])\n",
    "        if self.transformer == True:\n",
    "            x, _ = self.TransformerLayer(x)  # attn_output = torch.Size([512, 20, 128])\n",
    "        else:\n",
    "            x = self.LeakyReLU(x)\n",
    "        \n",
    "        x = torch.transpose(x, 1, 2)  # x.shape: torch.Size([6182, 128, 10])\n",
    "        x = self.batch2(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "\n",
    "        x, (hn, cn) = self.lstm2(x)  # torch.Size([6182, 10, 64])\n",
    "        if self.attention == True:\n",
    "            x, _ = self.attention_net(x, hn)\n",
    "        else:\n",
    "            x = x[:, -1]  # torch.Size([6182, 64])\n",
    "        x = self.batch3(x)  # torch.Size([6182, 64])\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        y_pred = self.out(x)\n",
    "        return y_pred\n",
    "\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        # lstm_output : [batch_size, n_step, n_hidden * num_directions(=2)], F matrix\n",
    "        # final_state : [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
    "\n",
    "        hidden = torch.cat((final_state[0], final_state[1]), dim=1).unsqueeze(\n",
    "            2)  # hidden : [batch_size, n_hidden * num_directions(=2), n_layer(=1)]\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2)  # [batch_size,sequence]\n",
    "        attn_weights = torch.nn.functional.softmax(attn_weights, 1)  # [batch_size,sequence]   # torch.Size([512, 20])\n",
    "        # context: [batch_size, n_hidden * num_directions(=2)]\n",
    "        output = torch.bmm(lstm_output.transpose(1, 2), attn_weights.unsqueeze(2)).squeeze(\n",
    "            2)  # [batch_size, n_hidden * num_directions(=2)]\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a73d237",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T11:16:06.775364Z",
     "start_time": "2022-01-30T11:16:06.524897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainx.shape:  torch.Size([10000, 20, 108])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from progressbar import ProgressBar\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "# trainx = torch.randn(10000,3,20,108)\n",
    "trainx = torch.randn(10000,20,108)\n",
    "trainy = torch.randn(10000,1)\n",
    "print(\"trainx.shape: \" , trainx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89897b57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T11:16:08.298438Z",
     "start_time": "2022-01-30T11:16:08.250676Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = Data.TensorDataset(trainx, trainy)\n",
    "batch_size = 1024\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=16,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d31b69",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-30T11:16:19.299Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlphaNet_LSTM_V1(\n",
      "  (batch): BatchNorm1d(2160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(108, 64, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (lstm2): LSTM(128, 32, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (batch2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (LeakyReLU): LeakyReLU(negative_slope=0.01)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 1/20 [00:02<00:38,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  loss:  1.1134240112304687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:04<00:37,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2  loss:  1.049112548828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:06<00:35,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3  loss:  1.03987265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:08<00:32,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4  loss:  1.010880987548828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:10<00:30,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5  loss:  0.9534799926757812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:12<00:28,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6  loss:  0.8197265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:14<00:26,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7  loss:  0.6417380981445312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:16<00:24,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8  loss:  0.466131982421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:18<00:22,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9  loss:  0.3212902099609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:20<00:20,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10  loss:  0.23362308044433594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:22<00:19,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11  loss:  0.1714110870361328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:25<00:17,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12  loss:  0.13443065032958984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [00:27<00:14,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13  loss:  0.11365663146972656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [00:29<00:12,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14  loss:  0.09025854949951172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [00:31<00:10,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15  loss:  0.08059934539794922\n"
     ]
    }
   ],
   "source": [
    "alphanet = AlphaNet_LSTM_V1(108,20,64,attention=True)\n",
    "# alphanet = Transformer_LSTM(3,108,20,64)\n",
    "alphanet = alphanet.cuda()\n",
    "# alphanet = torch.nn.parallel.DataParallel(alphanet)\n",
    "print(alphanet)\n",
    "total_length = trainx.shape[0]\n",
    "LR = 0.01\n",
    "loss_function = nn.MSELoss().cuda()\n",
    "optimizer = optim.Adam(alphanet.parameters(), lr=LR)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=2,gamma = 0.5)\n",
    "epoch_num = 20\n",
    "loss_list = []\n",
    "\n",
    "min_loss = float(\"inf\")\n",
    "for epoch in tqdm(range(epoch_num)):\n",
    "    total_loss = 0\n",
    "    for _, (inputs, outputs) in enumerate(train_loader):\n",
    "        inputs = Variable(inputs).float().cuda()\n",
    "        outputs = Variable(outputs).float().cuda()\n",
    "        optimizer.zero_grad() # noticed:  the grad return to zero before starting the loop\n",
    "        \n",
    "        # forward + backward +update\n",
    "        pred = alphanet(inputs.cuda())\n",
    "        pred = pred.cuda()\n",
    "        loss = loss_function(pred, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         lr_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "        total_loss += loss.item()\n",
    "    total_loss = total_loss * batch_size / total_length\n",
    "    print('Epoch: ', epoch + 1, ' loss: ', total_loss)\n",
    "    loss_list.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9322ba82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T11:05:36.255955Z",
     "start_time": "2022-01-30T11:04:59.537392Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer_LSTM(\n",
      "  (bn1): BatchNorm1d(6480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(2560, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (q_metrix): Linear(in_features=108, out_features=108, bias=True)\n",
      "  (k_metrix): Linear(in_features=108, out_features=108, bias=True)\n",
      "  (v_metrix): Linear(in_features=108, out_features=108, bias=True)\n",
      "  (MultiheadAttention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=108, out_features=108, bias=True)\n",
      "  )\n",
      "  (lstm): LSTM(108, 64, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (LeakyReLU): LeakyReLU(negative_slope=0.01)\n",
      "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 1/20 [00:02<00:43,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  loss:  1.1639129760742188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:04<00:44,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2  loss:  1.0448883239746094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:07<00:44,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3  loss:  1.035843603515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:10<00:43,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4  loss:  1.0386446533203124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:12<00:38,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5  loss:  1.0318886352539063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:15<00:34,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6  loss:  1.0264286254882813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:17<00:32,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7  loss:  1.0366453796386719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:20<00:30,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8  loss:  1.0355045043945312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [00:22<00:27,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9  loss:  1.0371146057128906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [00:25<00:24,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10  loss:  1.0378154479980468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [00:27<00:21,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11  loss:  1.037383349609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [00:29<00:19,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12  loss:  1.0386478637695313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f80ea88e680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python3/lib/python3.7/logging/__init__.py\", line 221, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt\n",
      " 60%|██████    | 12/20 [00:36<00:24,  3.05s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 266612, 266613, 266614, 266615, 266616, 266617, 266618) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m~/venv-jupyter/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/python3/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-45faea1ff538>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-jupyter/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 266612, 266613, 266614, 266615, 266616, 266617, 266618) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "alphanet = AlphaNet_LSTM_V1(108,20,64,attention=True,transformer = True)\n",
    "# alphanet = Transformer_LSTM(3,108,20,64)\n",
    "alphanet = alphanet.cuda()\n",
    "# alphanet = torch.nn.parallel.DataParallel(alphanet)\n",
    "print(alphanet)\n",
    "total_length = trainx.shape[0]\n",
    "LR = 0.01\n",
    "loss_function = nn.MSELoss().cuda()\n",
    "optimizer = optim.Adam(alphanet.parameters(), lr=LR)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=2,gamma = 0.5)\n",
    "epoch_num = 20\n",
    "loss_list = []\n",
    "\n",
    "min_loss = float(\"inf\")\n",
    "for epoch in tqdm(range(epoch_num)):\n",
    "    total_loss = 0\n",
    "    for _, (inputs, outputs) in enumerate(train_loader):\n",
    "        inputs = Variable(inputs).float().cuda()\n",
    "        outputs = Variable(outputs).float().cuda()\n",
    "        optimizer.zero_grad() # noticed:  the grad return to zero before starting the loop\n",
    "        \n",
    "        # forward + backward +update\n",
    "        pred = alphanet(inputs.cuda())\n",
    "        pred = pred.cuda()\n",
    "        loss = loss_function(pred, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         lr_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "        total_loss += loss.item()\n",
    "    total_loss = total_loss * batch_size / total_length\n",
    "    print('Epoch: ', epoch + 1, ' loss: ', total_loss)\n",
    "    loss_list.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa100a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3df0044b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T11:13:38.381233Z",
     "start_time": "2022-01-30T11:13:37.779965Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=108, nhead=2,batch_first=True)\n",
    "src = torch.rand(512, 20, 108)\n",
    "out = encoder_layer(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d15a5749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-30T11:13:44.152331Z",
     "start_time": "2022-01-30T11:13:44.137793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 20, 108])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
